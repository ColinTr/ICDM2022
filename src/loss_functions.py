import torch.nn.functional as F
import torch.nn as nn
import torch


def vime_loss(mask_pred, mask_true, feature_pred, batch_x_train):
    """
    Note that all the inputs should have values between 0 and 1.
    :param mask_pred: The predicted corruption mask, torch.Tensor of shape (n_samples, n_features).
    :param mask_true: The true corruption mask, torch.Tensor of shape (n_samples, n_features).
    :param feature_pred: The reconstructed values of x, torch.Tensor of shape (n_samples, n_features).
    :param batch_x_train: The original (uncorrupted) values of x, torch.Tensor of shape (n_samples, n_features).
    :return:
        mask_loss: The mean binary cross-entropy loss of the mask estimation.
        feature_loss: The mean binary cross-entropy loss of the feature estimation.
    """
    mask_loss = nn.BCELoss()(mask_pred, mask_true)
    feature_loss = nn.MSELoss()(feature_pred, batch_x_train)
    return mask_loss, feature_loss


def unsupervised_classification_loss(y_pred_1, y_pred_2, labels, eps=1e-7):
    """
    The intuition is that for each pair of samples, if their label is 1, we want the predicted
    probability of the same class to be high, and low if their label is 0.
    :param y_pred_1: The raw class predictions (before softmax) of the first list of samples,
                     torch.tensor of shape (n_samples, n_classes).
    :param y_pred_2: The raw class predictions (before softmax) of the second list of samples,
                     torch.tensor of shape (n_samples, n_classes).
    :param labels: The true labels of the samples (generated by cosine similarity),
                   the value are 0 when the class is different and 1 when it is the same.
                   torch.tensor of shape (n_samples,).
    :param eps: This float is used to 'clip' y_pred_proba's values that are 0, as computing log(0) is impossible.
    :return:
        Mean loss, float.
    """
    prob_1, prob_2 = F.softmax(y_pred_1, -1), F.softmax(y_pred_2, -1)  # Simple softmax of the output
    x = prob_1.mul(prob_2)  # We multiply the prediction of each vector between each other (so same shape is outputted)
    x = x.sum(1)  # We sum the results of each row. If the predictions of the same class were high, the result is close to 1
    return - torch.mean(labels.mul(x.add(eps).log()) + (1 - labels).mul((1 - x).add(eps).log()))  # BCE
